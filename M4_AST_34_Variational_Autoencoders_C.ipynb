{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-34/blob/main/M4_AST_34_Variational_Autoencoders_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "\n",
        "### Assignment: Implementation of Variational Autoencoders\n",
        "\n"
      ],
      "metadata": {
        "id": "IjzvN_UTwP3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "* understand variational autoencoders\n",
        "* implementing variational autoenocders using celebA dataset"
      ],
      "metadata": {
        "id": "8T2M9g2ZwTEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "WIeVvwgnMlL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Description\n",
        "\n",
        "CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. The images in this dataset cover large pose variations and background clutter. CelebA has large diversities, large quantities, and rich annotations, including\n",
        "\n",
        "* 10,177 number of identities,\n",
        "\n",
        "* 202,599 number of face images, and\n",
        "\n",
        "* 5 landmark locations, 40 binary attributes annotations per image.\n",
        "\n",
        "The dataset can be employed as the training and test sets for the following computer vision tasks: face attribute recognition, face detection, landmark (or facial part) localization, and face editing & synthesis."
      ],
      "metadata": {
        "id": "4qLldAC0MjO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information"
      ],
      "metadata": {
        "id": "-KNEowcry1Kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is dimensionality reduction?\n",
        "\n",
        "In machine learning, dimensionality reduction is the process of reducing the number of features that describe some data. This reduction is done either by selection (only some existing features are conserved) or by extraction (a reduced number of new features are created based on the old features) and can be useful in many situations that require low dimensional data (data visualisation, data storage, heavy computation…). Although there exists many different methods of dimensionality reduction, we can set a global framework that is matched by most of these methods.\n",
        "\n",
        "\n",
        "First, let’s call **encoder** the process that produce the\n",
        "**“new features”** representation from the **“old features”** representation (by selection or by extraction) and **decoder** the reverse process.\n",
        "\n",
        "Dimensionality reduction can then be interpreted as data compression where the encoder compress the data (from the initial space to the **encoded space**, also called **latent space**) whereas the decoder decompress them. Depending on the initial data distribution, the latent space dimension and the encoder definition, this compression can be lossy, meaning that a part of the information is lost during the encoding process and cannot be recovered when decoding.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/VAE_4.png\" width=700px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "**Principal components analysis (PCA)**\n",
        "\n",
        "One of the dimensionality reduction method is principal component analysis (PCA). The idea of PCA is to build new independent features that are linear combinations of the old features and so that the projections of the data on the subspace defined by these new features are as close as possible to the initial data (in term of euclidean distance). In other words, PCA is looking for the best linear subspace of the initial space (described by an orthogonal basis of new features) such that the error of approximating the data by their projections on this subspace is as small as possible.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/VAE_1.png\" width=500px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "\n",
        "As shown in the above figure, $x$ is a matrix which becomes a low-dimensional matrix $c$ through a transformation $W$ . Because this process is linear, the transpose of $W$ can be used to restore an $\\hat{x}$. PCA is to find a $W$ through SVD (singular value decomposition) so that the matrices $x$ and $x$ hat be as consistent as possible.\n",
        "<br><br>\n",
        "\n",
        "**Why we should use autoencoders?**\n",
        "\n",
        "Data denoising and Dimensionality reduction for data visualization are considered as two main interesting practical applications of autoencoders.\n",
        "Autoencoders can be used widely in **data compression** and transmission of compressed data. Autoencoders are a type of neural network that works in a self-supervised fashion. So in autoencoders, there are three main building blocks: **encoder**, **decoder**, and coder or **latent space**. So, first we feed the autoencoder with the data and then encoder encodes or simply extracts useful features of input data and stores it in latent space. And then the decoder does the same but in the opposite order.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/720/1*W_SdpxG7NxhjB2-u3CZZXg.png\" width=510px, height=280px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "\n",
        "As data compression is one of the main advantages of autoencoders, it can be successfully implemented in **data transmission problems**. For example:\n",
        "\n",
        "\n",
        "*   Imagine server uploads to internet only Latent Space Representation of the input image, depending on the request it gets and then, in the same manner, we download the Latent Space Representation to our mobile device and then we decode oour Latent Space Representation to get output result(for example image, video, audio files) even with better resolution/quality than it was in input.\n",
        "\n",
        "\n",
        "*   **Denoising data:** Quality loss of data (images/audio/video) is one main problem/issue in data transmission. Due to connection status or bandwidth, data such as image and audio can lose in quality, therefore the problem of denoising data arises. Denoising data is one of the advantages of autoencoders.\n",
        "<br><br>\n",
        "\n",
        "**What are the autoencoders?**\n",
        "\n",
        "\n",
        "Autoencoders are the type of unsupervised artificial neural networks. Autoencoder aims to learn representation for input data. Along with the reduction side, reconstruction is learned, where reconstruction side — decoder tries to reconstruct the input data from previously learned representation — Latent Space Representation with minimum loss. Usually, autoencoders consist of three-part: **encoder** — the part that includes input layer and hidden layer, **Latent Space** — this is where learned/compressed data is stored, and **decoder** — the part that starts from hidden layer and ends with output layer.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/VAE_2.png\" width=500px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "\n",
        "As shown in the above figure, $x$ is the input data and $(e(x))$ is the lower-dimension representation of input $x$ and $x’$ ($d(e(x))$ )is the reconstructed input data. The mapping of higher to lower dimensions can be linear or non-linear depending on the choice of the activation function.\n",
        "The entire network is usually trained as a whole. The loss function is usually either the `mean-squared error` or `cross-entropy` between the output and the input, known as the **reconstruction loss**, which penalizes the network for creating outputs different from the input.\n"
      ],
      "metadata": {
        "id": "d55iyMR-wFyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The problem with classical autoencoders**\n",
        "\n",
        "Standard autoencoders learn to generate compact representations and reconstruct their inputs well, but asides from a few applications like denoising autoencoders, they are fairly limited.\n",
        "\n",
        "The only constraint on the latent vector representation for traditional autoencoders is that latent vectors should be easily decodable back into the original image. As a result, the latent space $Z$ can become disjoint and non-continuous. **Variational autoencoders** try to solve this problem by using a probabilistic model of latent representations which understands the underlying relations better, helping in more effective generalization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RD9yS9UICwc8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psk_R-U_IexC"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "vInRrxGKIu64"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M4_AST_34_Variational_Autoencoders_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1s1L8cLMHpK"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNM5tQdH2c1E"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.utils as utils\n",
        "import torch.nn.init as init\n",
        "from torch.autograd import Variable\n",
        "import torchvision.utils as v_utils\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "import torchvision.utils as vutils\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAfD3lMJ4PeD"
      },
      "source": [
        "### Defining Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "263kz7ri4R-b"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 32\n",
        "learning_rate = 0.0005\n",
        "num_gpus = 1\n",
        "image_size= 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyoeWZxp20Am"
      },
      "source": [
        "### Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1AYQ8mYABOP"
      },
      "source": [
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "os.makedirs(\"celeba_gan\")\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
        "output = \"celeba_gan/data.zip\"\n",
        "gdown.download(url, output, quiet=True)\n",
        "\n",
        "with ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:\n",
        "    zipobj.extractall(\"celeba_gan\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the CelebA dataset"
      ],
      "metadata": {
        "id": "dcUkwPj0M-QR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zh3bLSE3Elb"
      },
      "source": [
        "# Loading the dataset using ImageFolder\n",
        "dataset = dset.ImageFolder(root=\"./celeba_gan\",\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_size),\n",
        "                               transforms.CenterCrop(image_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of images in the dataset\n",
        "len(dataset)"
      ],
      "metadata": {
        "id": "vCuxRCYCfIro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the CUDA\n",
        "\n"
      ],
      "metadata": {
        "id": "xvFvUVvJaC3Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFmW4izimMaM"
      },
      "source": [
        "Every Tensor in PyTorch has a **to()** member function. Its job is to put the tensor on which it's called to a certain device whether it be the CPU or a certain GPU.\n",
        "\n",
        "Input to the to function is a torch.device object which can be initialized with either of the following inputs.\n",
        "* cpu for CPU\n",
        "* cuda:0 for putting it on GPU number 0. Similarly, if your system has multiple GPUs, then the respective number would be considered while initializing the device.\n",
        "\n",
        "Generally, whenever you initialize a Tensor, it’s put on the CPU. You should move it to the GPU to make the related calculation faster.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Selected  : {device}')"
      ],
      "metadata": {
        "id": "8Lv2vNbLaDXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of CelebA dataset images"
      ],
      "metadata": {
        "id": "91Blx_cZZ0UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot some training images\n",
        "real_batch = next(iter(train_loader))\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0][:60], padding=2, normalize=True),(1,2,0)));"
      ],
      "metadata": {
        "id": "FHgCNRaCZx4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuTCDtZu3d02"
      },
      "source": [
        "### Variational Autoencoders(VAE)\n",
        "\n",
        "Another important category of autoencoders was introduced in 2013 by Diederik Kingma and Max Welling and quickly became one of the most popular types of autoencoders: variational autoencoders.\n",
        "\n",
        "They are quite different from all other autoencoders, in these particular ways:\n",
        "\n",
        "* They are probabilistic autoencoders, i.e, that their outputs are partly determined by chance, even after training.\n",
        "\n",
        "* They are generative autoencoders, i.e, that they can generate new instances that look like they were sampled from the training set.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/VAE_3.png\" width=570px/>\n",
        "</center>\n",
        "\n",
        "**Encoder**\n",
        "\n",
        "Encoder defines the approximate posterior distribution $q(z|x)$, which takes as input an observation and outputs a set of parameters for specifying the conditional distribution of the latent representation $z$. Here, simply model the distribution as a diagonal Gaussian, and the network outputs the **mean** and **log-variance** parameters of a factorized Gaussian. Output log-variance instead of the variance directly for numerical stability.\n",
        "\n",
        "\n",
        "**Decoder**\n",
        "\n",
        "Decoder defines the conditional distribution of the observation $q(x|z)$, which takes a latent sample as input and outputs the parameters for a conditional distribution of the observation. Model the latent distribution prior $P(z)$ as a unit Gaussian.\n",
        "\n",
        "**Reparameterization trick**\n",
        "\n",
        "To generate a sample $z$ for the decoder during training, we can sample from the latent distribution defined by the parameters outputted by the encoder, given an input observation $x$. However, this sampling operation creates a bottleneck because backpropagation cannot flow through a random node.\n",
        "\n",
        "To address this, use a reparameterization trick. In our example, we approximate $z$ using the decoder parameters and another parameter $ε$ as follows:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*t_LDtUnroBccBykfOO_5jA@2x.jpeg\" width=180px, height=30px/>\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "where $μ$ and $σ$ represent the mean and standard deviation of a Gaussian distribution respectively. They can be derived from the decoder output. The $ε$ can be thought of as a random noise used to maintain stochasticity of $z$. Generate $ε$ from a standard normal distribution $N(0,1)$. The latent variable $z$ is now generated by a function of $μ$, $σ$ and $ε$, which would enable the model to backpropagate gradients in the encoder through $μ$ and $σ$ respectively, while maintaining stochasticity through $ε$.\n",
        "\n",
        "**How VAE works?**\n",
        "\n",
        "The theoretical basis of VAE is the Gaussian mixture model (GMM). The difference is that our code is replaced by a continuous variable $z$, and $z$ follow standard normal distribution $N(0,1)$.\n",
        "\n",
        "For each sample $z$, there will be two variables $μ$ and $σ$, which respectively determine the mean and standard deviation of the Gaussian distribution corresponding to $z$, and then the accumulation of all Gaussian distributions in the integration domain becomes the original distribution $P(x)$\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/720/1*mBEMgLQbdUnPkvUB7PAB8Q@2x.jpeg\" width=250px, height=59px/>\n",
        "</center>\n",
        "\n",
        "Where $z~N(0,1)$, $x|z~N(μ(z)$, $σ(z))$, Since $P(z)$ is known, $P(x|z)$ is unknown, and $x|z~N(μ(z),σ(z))$. What we really need to solve is the expressions of $μ$ and $σ$, but $P(x)$ is so complex that μ and σ are difficult to be calculated, we need to introduce two neural networks(CNNs) to help us solve it.\n",
        "\n",
        "\n",
        "\n",
        "**Note:**\n",
        "1. Refer to the following [link](https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2) to understand more about VAE.\n",
        "\n",
        "2. Refer to the following [link](https://medium.com/analytics-vidhya/variational-inference-in-context-of-variational-autoencoders-vaes-e96bfe859980) for the implementation of VAE."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining the CNN model for Variational Autoencoders"
      ],
      "metadata": {
        "id": "kSUGPCpcYcgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter channel size\n",
        "filter_size = 64\n",
        "\n",
        "# Latent Variable\n",
        "latent_dim = 512"
      ],
      "metadata": {
        "id": "PeuWyG2KYqTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_zwPNOT5NbP"
      },
      "source": [
        "# Defining the VAE class\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(VAE,self).__init__()\n",
        "      # Define the convolutional layers for encoder and decoder\n",
        "      # Encoder part of the variational autoencoders\n",
        "      self.encode_layer = nn.Sequential(OrderedDict([\n",
        "                        ('conv1',nn.Conv2d(3, filter_size, kernel_size = 4, stride = 2, padding = 1)),\n",
        "                        ('bn1',nn.BatchNorm2d(filter_size)),\n",
        "                        ('relu1',nn.ReLU(0.2)),\n",
        "                        ('conv2',nn.Conv2d(filter_size, filter_size * 2, kernel_size = 4, stride = 2, padding = 1)),\n",
        "                        ('bn2',nn.BatchNorm2d(filter_size * 2)),\n",
        "                        ('relu2',nn.ReLU(0.2)),\n",
        "                        ('conv3', nn.Conv2d(filter_size * 2, filter_size*4,  kernel_size = 4, stride = 2, padding = 1)),\n",
        "                        ('bn3', nn.BatchNorm2d(filter_size * 4)),\n",
        "                        ('relu3', nn.ReLU(0.2))\n",
        "        ]))\n",
        "      # Decoder part of the variational autoencoders\n",
        "      self.decode_layer = nn.Sequential(OrderedDict([\n",
        "                    ('deconv1', nn.ConvTranspose2d(filter_size*4, filter_size*2, kernel_size = 4, stride = 2, padding = 1)),\n",
        "                    ('deconv1_bn', nn.BatchNorm2d(filter_size * 2)),\n",
        "                    ('leakyrelu1', nn.LeakyReLU(0.2)),\n",
        "                    ('deconv2', nn.ConvTranspose2d(filter_size * 2, filter_size, kernel_size = 4, stride = 2, padding = 1)),\n",
        "                    ('deconv2_bn', nn.BatchNorm2d(filter_size)),\n",
        "                    ('leakyrelu1', nn.LeakyReLU(0.2)),\n",
        "                    ('deconv3', nn.ConvTranspose2d(filter_size, 3, kernel_size = 4, stride = 2, padding = 1)),\n",
        "                    ('tanh3', nn.Tanh())\n",
        "      ]))\n",
        "      # Fully connected layers\n",
        "      self.fc1 = nn.Linear(filter_size * 4 * 8 * 8, latent_dim)\n",
        "      self.fc2 = nn.Linear(filter_size * 4 * 8 * 8, latent_dim)\n",
        "      self.d1 = nn.Linear(latent_dim, filter_size * 4 * 8 * 8)\n",
        "      self.z_size = latent_dim\n",
        "      self.d_max = filter_size * 4\n",
        "\n",
        "    def encode(self,x):\n",
        "      \"\"\"Defines the encoder's layers.\n",
        "      Args:\n",
        "          inputs -- batch from the dataset\n",
        "          latent_dim -- dimensionality of the latent space\n",
        "\n",
        "      Returns:\n",
        "          mu -- learned mean\n",
        "          sigma -- learned standard deviation (log variance)\n",
        "       \"\"\"\n",
        "      x = self.encode_layer(x)\n",
        "      x = x.view(x.shape[0], -1)\n",
        "      mu = self.fc1(x)\n",
        "      log_var = self.fc2(x)\n",
        "      return mu, log_var\n",
        "\n",
        "    # Sampling function\n",
        "    # Define the function to provide the Gaussian noise input along with the mean (mu) and standard deviation (sigma)\n",
        "    # of the encoder's output defined the equation above to combine these\n",
        "    def reparameterize(self, mu, log_var):\n",
        "      # Standard deviation\n",
        "      std = torch.exp(0.5 * log_var)\n",
        "      # define the epsilon values with randn\n",
        "      eps = torch.randn_like(std)\n",
        "\n",
        "      return eps.mul(std).add_(mu)\n",
        "\n",
        "    def decode(self,x):\n",
        "      \"\"\"Defines the decoder layers.\n",
        "            Args:\n",
        "              inputs -- output of the encoder\n",
        "              conv_shape -- shape of the features before flattening\n",
        "\n",
        "            Returns:\n",
        "              tensor containing the decoded output\n",
        "      \"\"\"\n",
        "      x = x.view(x.shape[0], self.z_size)\n",
        "      x = self.d1(x)\n",
        "      x = x.view(x.shape[0], self.d_max, 8, 8)\n",
        "      x = self.decode_layer(x)\n",
        "      return x\n",
        "\n",
        "    # Forward funcion of the model\n",
        "    def forward(self, x):\n",
        "      \"\"\"Defines the VAE model\n",
        "          Args:\n",
        "            encoder -- the encoder model\n",
        "            decoder -- the decoder model\n",
        "        Returns:\n",
        "            the complete VAE model\n",
        "      \"\"\"\n",
        "      mu, log_var = self.encode(x)\n",
        "      mu = mu.squeeze()\n",
        "      log_var = log_var.squeeze()\n",
        "      z = self.reparameterize(mu, log_var)\n",
        "      return self.decode(z.view(-1, self.z_size, 1, 1)), mu, log_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an Instance for the model"
      ],
      "metadata": {
        "id": "Z11g7GkLgOEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae = VAE().cuda()\n",
        "print(vae)"
      ],
      "metadata": {
        "id": "He82OMvLffWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUd_ppH068aU"
      },
      "source": [
        "### Loss Function & Optimizer\n",
        "\n",
        "**VAE Loss Function**\n",
        "\n",
        "The cost function is composed of two parts.\n",
        "\n",
        "* The first is the usual **reconstruction loss** here, we are using Mean Square Error loss function that pushes the autoencoder to reproduce its inputs.\n",
        "\n",
        "* The second is the latent loss that pushes the autoencoder to have codings that look as though they were sampled from a simple Gaussian distribution.\n",
        "\n",
        "The latent loss can be computed using Equation:\n",
        "\n",
        "Variational autoencoder’s latent loss,\n",
        "\n",
        "$$L = -\\frac{1}{2}\\Sigma_{i=1}^{n}[1 + log(\\sigma_i^2) - \\sigma_i^2 - \\mu_i^2]$$\n",
        "\n",
        "where, $L$ is the latent loss, $n$ is the codings’ dimensionality, and $μ_i$ and $σ_i$ are the mean and standard deviation of the $i^{th}$ component of the codings. The vectors $μ$ and $σ$ are output by the encoder, as shown in\n",
        "the above figure.\n",
        "\n",
        "A common tweak to the variational autoencoder’s architecture is to make the encoder output $γ = log(σ^2)$ rather than $σ$. The latent loss can then be computed as:\n",
        "\n",
        "$$L = -\\frac{1}{2}\\Sigma_{i=1}^{n}[1 + \\gamma_i - exp(\\gamma_i) - \\mu_i^2]$$\n",
        "\n",
        "This divergence is a way to measure how “different” two probability distributions are from each other. By minimizing it, the distributions will come closer to the origin of the latent space.\n",
        "\n",
        "Hence the training loss of VAE is defined as the sum of these the reconstruction loss and the latent loss\n",
        "\n",
        "<center>\n",
        "$ Loss = reconstruction \\ loss (MSE) + latent \\ loss$\n",
        "</center>\n",
        "\n",
        "This approach is more numerically stable and speeds up training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9asmeKuK46Q"
      },
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "        \"\"\" Computes the reconstruction loss (MSE) and Kullback-Leibler Divergence (KLD)\n",
        "          Args:\n",
        "            inputs -- batch from the dataset\n",
        "            outputs -- output of the Sampling layer\n",
        "            mu -- mean\n",
        "            sigma -- standard deviation\n",
        "\n",
        "          Returns:\n",
        "            KLD loss + reconstructed loss\n",
        "        \"\"\"\n",
        "        kld_weight = 1\n",
        "        recons_loss = F.mse_loss(recon_x, x, reduction=\"sum\")\n",
        "        kld_loss = torch.sum(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1), dim = 0)\n",
        "        loss = recons_loss + kld_weight * kld_loss\n",
        "        return loss\n",
        "\n",
        "# Defining the adam optimizer\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr = learning_rate, betas=(0.5, 0.999), weight_decay=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q1Dk3GX7mND"
      },
      "source": [
        "### Train the VAE Model\n",
        "\n",
        "**Note:** The VAE model takes around 1 hour for training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First switch the module mode to vae.train() so that new weights can be learned after every epoch.\n",
        "vae.train()\n",
        "\n",
        "# Iterate through the number of epochs\n",
        "for epoch in range(epochs):\n",
        "    running_train_loss = 0\n",
        "\n",
        "    # Get images from trainloader\n",
        "    for idx, data in enumerate(train_loader):\n",
        "\n",
        "        # Converting the data to GPU for faster execution\n",
        "        image = Variable(data[0]).to(device)\n",
        "\n",
        "        # zero out the gradients from the preivous step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Do forward pass on the current mini batch\n",
        "        recon_images, mu, log_var = vae(image)\n",
        "\n",
        "        # Compute loss on the current mini batch\n",
        "        loss = loss_function(recon_images, image, mu, log_var)\n",
        "\n",
        "        # Do backward pass. That is compute all the gradients for the current minibatch\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters using the gradients with the learning rate\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    # Calculate average training loss value\n",
        "    train_loss_value = running_train_loss/len(train_loader.dataset)\n",
        "    print(\"Epoch[{}/{}] Loss (Reconstruction + KLD): {:.3f}\".format(epoch+1, epochs, train_loss_value))"
      ],
      "metadata": {
        "id": "D7X9mckuDdlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the VAE Model"
      ],
      "metadata": {
        "id": "jrILUac2gM-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model model to .eval\n",
        "vae.eval()\n",
        "\n",
        "def to_image(x):\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = to_image(img)\n",
        "    img = img.numpy()\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))"
      ],
      "metadata": {
        "id": "gITxEZkqlePT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model by taking grid of images from training set and passing to the model and reconstructing the images back\n",
        "def visualise_output(images, model):\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Converting images to gpu\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        images, _, _ = model(images)\n",
        "        images = images.cpu()\n",
        "\n",
        "        # Making a grid of images for visualization\n",
        "        images = to_image(images)\n",
        "        np_imagegrid = torchvision.utils.make_grid(images[0:30], 10, 5).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "images, _ = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "lutdahSTlkDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdisrI3nq6d0"
      },
      "source": [
        "# First visualise the original images\n",
        "print('Original images')\n",
        "fig = plt.figure(figsize=(20, 6))\n",
        "show_image(torchvision.utils.make_grid(images[0:30], 10, 5))\n",
        "plt.show()\n",
        "\n",
        "# Reconstruct and visualise the images using the vae model\n",
        "print('VAE reconstruction:')\n",
        "fig = plt.figure(figsize=(20, 6))\n",
        "visualise_output(images, vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNaLFvpbcgm5"
      },
      "source": [
        "#### Consider the following statements about Autoencoders and answer Q1.\n",
        "\n",
        "\n",
        "A. The Encoder learns to compress (reduce) the input data into an encoded representation.\n",
        "\n",
        "B. The decoder learns to reconstruct the original data from the encoded representation to be as close to the original input as possible.\n",
        "\n",
        "C. The Bottleneck/Latent space is the layer that contains the compressed representation of the input data.\n",
        "\n",
        "D. Reconstruction loss(MSE) measures how well the decoder is performing, i.e. measures the difference between the encoded and decoded vectors, lesser the better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "mIPZ4gnOcgnN"
      },
      "source": [
        "#@title Q.1. Which of the above statements is/are true regarding Autoencoders?\n",
        "Answer1 = \"\" #@param [\"\",\"Only A\", \"Only C\", \"Only D\", \"Only A and B\",\"Only C and D\", \"Only B and D\", \"A, B, C and D\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_F3RfGojqZDc"
      },
      "source": [
        "#@title Q.2. ELBO (evidence lower bound) is a key concept in Variational Bayesian Methods. It transforms inference problems, which are always intractable, into optimization problems that can be solved with, for example, gradient-based methods.\n",
        "Answer2 = \"\" #@param [\"\",\"TRUE\", \"FALSE\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}